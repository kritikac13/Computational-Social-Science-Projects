{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Computational Social Science] Project 4: Unsupervised Learning\n",
    "\n",
    "**Enter your Name:**__Kritika Chopra__________\n",
    "\n",
    "*Semester:* Fall 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description and Preprocessing\n",
    "\n",
    "For this project, you will explore data from the [National Health and Nutrition Examination Survey](https://www.kaggle.com/cdc/national-health-and-nutrition-examination-survey?select=questionnaire.csv). NHANES is a unique study that combines survey methodology with in-person medical examinations to create a dataset with demographic information, health indicators, and health outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the data and doing some preliminary preprocessing for you. We import some libraries that will be helpful as well. 'SEQN' is the ID number for each respondent, and 'HSD010' will be our target outcome. [HSD010](https://wwwn.cdc.gov/Nchs/Nhanes/2013-2014/HSQ_H.htm#HSD010) asks for the respondent's self reported health condition, which can range from \"excellent\" to \"poor.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "sns.set_style(\"darkgrid\")\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'SEQN'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'SEQN'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m nhanes \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnhanes.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Get the ID numbers for each observation (seqn)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m seqn \u001b[38;5;241m=\u001b[39m nhanes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSEQN\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Get the target, \"self-reported health condition,\" HSD010\u001b[39;00m\n\u001b[0;32m      6\u001b[0m hsd010 \u001b[38;5;241m=\u001b[39m nhanes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHSD010\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'SEQN'"
     ]
    }
   ],
   "source": [
    "# Load nhanes data\n",
    "nhanes = pd.read_csv('nhanes.csv')\n",
    "# Get the ID numbers for each observation (seqn)\n",
    "seqn = nhanes['SEQN']\n",
    "# Get the target, \"self-reported health condition,\" HSD010\n",
    "hsd010 = nhanes['HSD010']\n",
    "# Drop SEQN from the dataframe and then apply the standard scaler\n",
    "nhanes = nhanes.drop(['SEQN', 'HSD010'], axis = 1)\n",
    "nhanes_scaled = pd.DataFrame(StandardScaler().fit_transform(nhanes),\n",
    "                             columns = nhanes.columns)\n",
    "# Add the ID and target back in\n",
    "nhanes_scaled['SEQN'] = seqn\n",
    "nhanes_scaled['HSD010'] = hsd010\n",
    "nhanes_scaled = nhanes_scaled.set_index('SEQN')\n",
    "nhanes_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data. Below we visualize boxplots of family income to federal poverty line ratio ('INDFMPIR') and self-reported health condition. Notice how there are some clear patterns (the lower the ratio, the lower reported health condition), but it's not a perfect separation. We have 240+ features in our dataset, and we likely have several features in our dataset that highly correlate with our family income-poverty line ratio measure - PCA will help us simplify these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary version of hsd010 where 1-3 are \"good\" and 4-5 are \"poor\"\n",
    "nhanes_scaled['HSD010_binary'] = hsd010_binary = nhanes_scaled['HSD010'].replace(\n",
    "    [1, 2, 3, 4, 5], ['good', 'good', 'good', 'poor', 'poor']) \n",
    "# Recode the original hsd010 with the string labels\n",
    "nhanes_scaled['HSD010'] = nhanes_scaled['HSD010'].replace(\n",
    "    [1, 2, 3, 4, 5], ['excellent', 'very good', 'good', 'fair', 'poor'])\n",
    "# Boxplot of hsd010\n",
    "ax = sns.boxplot(x = 'HSD010', y = 'INDFMPIR', data = nhanes_scaled)\n",
    "ax.set(xlabel = \"Self Reported Health Condition\",\n",
    "      ylabel = \"Family Income to Poverty Line Ratio\")\n",
    "ax.set_title(\"Boxplot of Family Income to Poverty Line Ratio by Self-Reported Health Status\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of hsd010_binary\n",
    "ax = sns.boxplot(x = 'HSD010_binary', y = 'INDFMPIR', data = nhanes_scaled)\n",
    "ax.set(xlabel = \"Self Reported Health Condition\",\n",
    "      ylabel = \"Family Income to Poverty Line Ratio\")\n",
    "ax.set_title(\"Boxplot of Family Income to Poverty Line Ratio by Binary Self-Reported Health Status\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Family income also is not necessarily well correlated with measured health outcomes. See below where we look at the relationship between Body Mass Index (BMI) and the family income-poverty line ratio, and shade points by self-reported health condition. It's hard to find a clear pattern - this is where clustering may come in handy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x = \"INDFMPIR\", y = \"BMXBMI\", hue = \"HSD010\", palette = \"tab10\", data = nhanes_scaled)\n",
    "ax.set(xlabel = \"Income to Poverty Line Ratio\",\n",
    "      ylabel = \"Body Mass Index\")\n",
    "ax.set_title(\"BMI v. Income-Poverty Ratio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move to working on unsupervised methods, we'll drop our target variables again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhanes_scaled = nhanes_scaled.drop(['HSD010', 'HSD010_binary'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct a Principal Component Analysis (PCA) of the nhanes data. The data has already been prepared for you, so you can work directly on nhanes_scaled. Be sure to do the following:\n",
    "\n",
    "- Choose the number of components and provide 1-2 sentences about your choice of the number of components. \n",
    "- Plot a barplot of the variation explained by each component. *Hint*: look at the attributes associated with your model. \n",
    "- Choose how many components you will use to fit a supervised learning model and provide 1-2 sentences to explain that choice.\n",
    "- Plot a 2D scatterplot of the first two components and provide 1-2 sentences analyzing the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train PCA and Discuss Number of Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Perform PCA\n",
    "pca_nhanes = PCA()\n",
    "nhanes_pca_result = pca_nhanes.fit_transform(nhanes_scaled)\n",
    "\n",
    "# Step 2: Calculate explained variance\n",
    "nhanes_explained_variance = pd.DataFrame({\n",
    "    \"Principal Component\": [f\"PC{i+1}\" for i in range(len(pca_nhanes.explained_variance_ratio_))],\n",
    "    \"Explained Variance Ratio\": pca_nhanes.explained_variance_ratio_,\n",
    "    \"Cumulative Variance Ratio\": pca_nhanes.explained_variance_ratio_.cumsum()\n",
    "})\n",
    "\n",
    "# Display the total number of PCs\n",
    "total_pcs = len(pca_nhanes.explained_variance_ratio_)\n",
    "print(f\"Total number of PCs: {total_pcs}\")\n",
    "\n",
    "# Step 3: Determine the number of components explaining 80% variance\n",
    "n_components_80_nhanes = (nhanes_explained_variance[\"Cumulative Variance Ratio\"] < 0.80).sum() + 1\n",
    "print(f\"Number of components for 80% cumulative variance: {n_components_80_nhanes}\")\n",
    "\n",
    "# Step 4: Reduce PCA results to selected components\n",
    "nhanes_reduced_pca_df = pd.DataFrame(\n",
    "    nhanes_pca_result[:, :n_components_80_nhanes],\n",
    "    columns=[f\"PC{i+1}\" for i in range(n_components_80_nhanes)],\n",
    "    index=nhanes_scaled.index  # Maintain the same index as the original data\n",
    ")\n",
    "\n",
    "# Step 5: Display explained variance for selected components\n",
    "print(\"\\nExplained Variance for Selected Components:\")\n",
    "print(nhanes_explained_variance.iloc[:n_components_80_nhanes])\n",
    "\n",
    "# Save reduced PCA dataset and explained variance to CSV (optional)\n",
    "nhanes_reduced_pca_df.to_csv(\"nhanes_reduced_pca_dataset.csv\", index=True)\n",
    "nhanes_explained_variance.to_csv(\"nhanes_pca_explained_variance.csv\", index=False)\n",
    "\n",
    "# Step 6: Extract PCA loadings (contributions of features to PCs)\n",
    "loadings = pd.DataFrame(\n",
    "    pca_nhanes.components_.T,  # Transpose to align features with components\n",
    "    index=nhanes_scaled.columns,  # Original feature names\n",
    "    columns=[f\"PC{i+1}\" for i in range(pca_nhanes.n_components_)]  # Principal Component names\n",
    ")\n",
    "\n",
    "# Sort features by their absolute contribution to PC1\n",
    "pc1_loadings = loadings[\"PC1\"].sort_values(key=abs, ascending=False)\n",
    "\n",
    "# Display the top contributing features to PC1\n",
    "print(\"\\nTop features contributing to PC1:\")\n",
    "print(pc1_loadings.head(10))  # Adjust the number of features to display as needed\n",
    "\n",
    "# Save PC1 loadings to CSV (optional)\n",
    "pc1_loadings.to_csv(\"pc1_loadings.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is being applied to scaled datasets in order to reduce dimensionality. Here we are preserving 80% of the variance. PCA helps in identifying key features and contributing to each principal component and gives the reduced dataset, explained variance, and feature loadings. This approach highlights its most significant variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are  241 principal components. To retain 80% of the variance, 86 components were needed to reduce dimensionality. The first 5 components explain ~27.44% of the variance. Here PC1 alone contributes ~10.2%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barplot of Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the reduced PCA dataset and explained variance data\n",
    "reduced_pca_df = pd.read_csv(\"C:\\\\Users\\\\chopr\\\\Computational-Social-Science-Training-Program\\\\Projects\\\\Project 4\\\\nhanes_reduced_pca_dataset.csv\", index_col=0)  # Replace with your file path\n",
    "explained_variance = pd.read_csv(\"C:\\\\Users\\\\chopr\\\\Computational-Social-Science-Training-Program\\\\Projects\\\\Project 4\\\\nhanes_pca_explained_variance.csv\")  # Replace with your file path\n",
    "\n",
    "# Extract the explained variance ratio for the reduced components\n",
    "explained_variance_reduced = explained_variance.iloc[:len(reduced_pca_df.columns)]\n",
    "\n",
    "# Plot the bar plot for explained variance ratio\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(\n",
    "    explained_variance_reduced[\"Principal Component\"],\n",
    "    explained_variance_reduced[\"Explained Variance Ratio\"],\n",
    "    color='blue',\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title(\"Explained Variance by Principal Component\")\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate the cumulative explained variance\n",
    "cumulative_variance = explained_variance_reduced[\"Explained Variance Ratio\"].cumsum()\n",
    "\n",
    "# Plot the cumulative explained variance as a line graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(\n",
    "    explained_variance_reduced[\"Principal Component\"],\n",
    "    cumulative_variance,\n",
    "    marker='o',\n",
    "    linestyle='--',\n",
    "    color='green'\n",
    ")\n",
    "plt.title(\"Cumulative Explained Variance by Principal Component\")\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.ylabel(\"Cumulative Explained Variance Ratio\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Inspect the explained variance ratio for debugging\n",
    "print(\"Explained Variance Ratios:\")\n",
    "print(explained_variance_reduced[\"Explained Variance Ratio\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the code is analyzing the results of PCA on a dataset. Explained variance and cumulative explained is being visualized. This is done to determine the optimal number of principal components needed to capture portion of the original data's variance. Dimensionality reduction and feature importance in the dataset is being addressed by the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explained variance ratio for each principal component is shown by the bar plot. Here the X-axis represents the components and the Y-axis represents the proportion of variance explained. The decline in bar heights indicates that the first few components are capturing most of the dataset's variance by helping in dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many components will you use to fit a supervised learning model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Load and preprocess the dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\chopr\\\\Computational-Social-Science-Training-Program\\\\Projects\\\\Project 4\\\\nhanes_reduced_pca_dataset.csv\")  # Replace with your dataset\n",
    "numeric_data = data.select_dtypes(include=[np.number])  # Select numeric columns only\n",
    "numeric_data = numeric_data.dropna()  # Drop missing values\n",
    "\n",
    "# Step 2: Scale the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(numeric_data)\n",
    "\n",
    "# Step 3: Apply PCA\n",
    "pca = PCA()\n",
    "pca_result = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Step 4: Extract explained variance\n",
    "explained_variance = pd.DataFrame({\n",
    "    \"Principal Component\": [f\"PC{i+1}\" for i in range(len(pca.explained_variance_ratio_))],\n",
    "    \"Explained Variance Ratio\": pca.explained_variance_ratio_,\n",
    "    \"Cumulative Variance Ratio\": np.cumsum(pca.explained_variance_ratio_)\n",
    "})\n",
    "\n",
    "# Step 5: Determine the number of components explaining 60% variance\n",
    "n_components_60 = (explained_variance[\"Cumulative Variance Ratio\"] < 0.60).sum() + 1\n",
    "\n",
    "# Step 6: Reduce PCA results to selected components\n",
    "reduced_pca_60_df = pd.DataFrame(\n",
    "    pca_result[:, :n_components_60],  # Select the first 'n_components_60' components\n",
    "    columns=[f\"PC{i+1}\" for i in range(n_components_60)],  # Label components as PC1, PC2, ...\n",
    "    index=numeric_data.index  # Maintain the same index as the original data\n",
    ")\n",
    "\n",
    "# Step 7: Display results\n",
    "print(f\"Number of components for 60% cumulative variance: {n_components_60}\")\n",
    "print(f\"Reduced PCA dataset shape: {reduced_pca_60_df.shape}\")\n",
    "print(\"Explained Variance for Selected Components:\")\n",
    "print(explained_variance.iloc[:n_components_60])\n",
    "\n",
    "# Step 8: Save the reduced dataset for 60% cumulative variance to CSV (optional)\n",
    "reduced_pca_60_df.to_csv(\"reduced_pca_60_dataset.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code analyzes the results of a Principal Component Analysis (PCA) on a dataset. It visualizes the explained variance and cumulative explained variance to determine the optimal number of principal components needed to capture a significant portion of the original data's variance. This helps in understanding the dimensionality reduction and feature importance in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are  52 principal components. To retain 60% of the variance, 52 components were needed to reduce dimensionality. The first 5 components explain ~5.99% of the variance. Here PC1 alone contributes ~1.395%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Scatterplot of the first two components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a scatter plot for the first two principal components\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(\n",
    "    reduced_pca_df[\"PC1\"], \n",
    "    reduced_pca_df[\"PC2\"], \n",
    "    alpha=0.6, \n",
    "    edgecolor='k'\n",
    ")\n",
    "\n",
    "# Add plot details\n",
    "plt.title(\"2D Scatter Plot of the First Two Principal Components\")\n",
    "plt.xlabel(\"Principal Component 1 (PC1)\")\n",
    "plt.ylabel(\"Principal Component 2 (PC2)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D scatter plot is being created to visualize the first two principal components obtained from a PCA analysis. The X-axis represents PC1 and the y-axis represents PC2. We can see the distribution and relationships between the first two principal components through this scatter plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data points are distributed across the plot where the variation is being captured by these two components. Most clusters are near the origin whereas a few spread out. This indicates potential outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Choose a clustering algorithm and explain it in 1-2 sentences.\n",
    "- Cluster the nhanes data. Detail any choice you need to make with regards to number of clusters, and how you arrived at that choice. For instance, you might use the [elbow method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)) if you choose k-means.\n",
    "- Plot your clusters on top of BMI v. Income Poverty Ratio Plot. Describe what you see in 1-2 sentences.\n",
    "- Retrain the clustering algorithm, but this time use your PCA results instead of the original dataframe. Plot the clusters on top of the 2D PCA scatterplot from the previous step. Describe your results in 1-2 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a Clustering Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We are choosing K-Means. Therefore we will use elbow method to find out number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster nhanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load the raw NHANES dataset\n",
    "nhanes = pd.read_csv(\"nhanes.csv\")  # Replace with the correct file path\n",
    "\n",
    "# Step 2: Separate the SEQN column\n",
    "seqn = nhanes['SEQN']  # Keep SEQN as it is\n",
    "data_to_scale = nhanes.drop(columns=['SEQN'])  # Drop SEQN for scaling\n",
    "\n",
    "# Step 3: Scale the remaining data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "nhanes_scaled = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "\n",
    "# Step 4: Add SEQN back to the scaled dataset\n",
    "nhanes_scaled.insert(0, 'SEQN', seqn)  # Insert SEQN as the first column\n",
    "\n",
    "# Save the corrected scaled dataset\n",
    "nhanes_scaled.to_csv(\"nhanes_scaled_corrected.csv\", index=False)\n",
    "\n",
    "# Step 5: Proceed with K-means clustering\n",
    "# Load the corrected scaled dataset\n",
    "nhanes_scaled = pd.read_csv(\"nhanes_scaled_corrected.csv\")\n",
    "\n",
    "# Elbow Method to determine optimal number of clusters\n",
    "wcss = []  # Within-Cluster-Sum-of-Squares\n",
    "max_clusters = 10  # Check for up to 10 clusters\n",
    "\n",
    "for i in range(1, max_clusters + 1):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42, n_init=10)\n",
    "    kmeans.fit(nhanes_scaled.drop(columns=['SEQN']))  # Exclude SEQN for clustering\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the Elbow Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, max_clusters + 1), wcss, marker='o', linestyle='--')\n",
    "plt.title('Elbow Method for Optimal Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.xticks(range(1, max_clusters + 1))\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Choose optimal number of clusters (e.g., 3) and fit KMeans\n",
    "optimal_clusters = 3  # Adjust based on the Elbow Method\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(nhanes_scaled.drop(columns=['SEQN']))\n",
    "\n",
    "# Add cluster labels to the dataset\n",
    "nhanes_scaled_with_clusters = nhanes_scaled.copy()\n",
    "nhanes_scaled_with_clusters['Cluster'] = cluster_labels\n",
    "\n",
    "# Save the clustered dataset\n",
    "nhanes_scaled_with_clusters.to_csv(\"nhanes_scaled_with_clusters_corrected.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making an elbow plot to find out the number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal number of clusters are 4 as we see curve near 3 and 4. We can choose either 3 or 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot your clusters on top of the BMI v. Income Poverty Ratio Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure the relevant columns exist in the dataset\n",
    "if 'BMXBMI' in nhanes_scaled_with_clusters.columns and 'INDFMPIR' in nhanes_scaled_with_clusters.columns:\n",
    "    # Extract the relevant columns for plotting\n",
    "    bmi = nhanes_scaled_with_clusters['BMXBMI']\n",
    "    income_poverty_ratio = nhanes_scaled_with_clusters['INDFMPIR']\n",
    "    clusters = nhanes_scaled_with_clusters['Cluster']\n",
    "    \n",
    "    # Create a scatter plot of BMI vs. Income Poverty Ratio colored by cluster\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(bmi, income_poverty_ratio, c=clusters, cmap='viridis', alpha=0.7, edgecolor='k')\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.title('Clusters on BMI vs. Income Poverty Ratio', fontsize=16)\n",
    "    plt.xlabel('BMI', fontsize=14)\n",
    "    plt.ylabel('Income Poverty Ratio', fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"The required columns 'BMXBMI' and 'INDFMPIR' are not in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a scatter plot of BMI vs. Income Poverty Ratio which is color-coded by cluster assignments. Clusters are being displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BMI values range from approximately -2 to 5 and the Income Poverty Ratio is between -1.5 and 1.5. Clusters in the plot overlap with points of different colors appearing close together, indicating poor separability in this feature space. Some clusters might capture specific relationships between BMI and income poverty but the blurred boundaries suggest high variability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain the clustering algorithm on PCA components and plot clusters on your 2D scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load the scaled NHANES dataset\n",
    "nhanes_scaled = pd.read_csv(\"nhanes_scaled_corrected.csv\")  # Replace with the correct file path\n",
    "\n",
    "# Step 2: Perform PCA for dimensionality reduction to 2 components\n",
    "pca = PCA(n_components=2)  # Reduce to 2 dimensions for visualization\n",
    "nhanes_pca_result = pca.fit_transform(nhanes_scaled.drop(columns=['SEQN']))  # Exclude SEQN\n",
    "\n",
    "# Create a DataFrame for the PCA-transformed data\n",
    "nhanes_pca_df = pd.DataFrame(\n",
    "    nhanes_pca_result, \n",
    "    columns=['PC1', 'PC2']\n",
    ")\n",
    "nhanes_pca_df['SEQN'] = nhanes_scaled['SEQN']  # Reattach SEQN\n",
    "\n",
    "# Step 3: Retrain K-means clustering on the PCA-reduced dataset\n",
    "optimal_clusters = 3  # Adjust based on the Elbow Method\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(nhanes_pca_df[['PC1', 'PC2']])\n",
    "\n",
    "# Add cluster labels to the PCA DataFrame\n",
    "nhanes_pca_df['Cluster'] = cluster_labels\n",
    "\n",
    "# Step 4: Plot the clusters on a 2D scatter plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(\n",
    "    nhanes_pca_df['PC1'], \n",
    "    nhanes_pca_df['PC2'], \n",
    "    c=nhanes_pca_df['Cluster'], \n",
    "    cmap='viridis', \n",
    "    alpha=0.7, \n",
    "    edgecolor='k'\n",
    ")\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.title('Clusters on PCA-Reduced Components (2D Scatter)', fontsize=16)\n",
    "plt.xlabel('Principal Component 1 (PC1)', fontsize=14)\n",
    "plt.ylabel('Principal Component 2 (PC2)', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is used to reduce the dataset to two principal components for visualization.\n",
    "K-means is used on the PCA-transformed data to group the data into clusters based on the optimal number of clusters. Here we are using 3 clusters.  The clusters are plotted in a 2D scatterplot using the two principal components in order to visualize the separation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Scatter plot shows clusters formed using K-means after reducing the dataset to two principal components with PCA. There are three distinct clusters with groups of data points with similar characteristics in the PCA-transformed feature. The clear separation of clusters shows differentiation among the data points based on the selected components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to predict! Do the following:\n",
    "\n",
    "- Choose either HSD010 or HSD010\\_binary as your target outcome.  \n",
    "- Train a neural network using the original features. Much of the code to train a basic neural net has been set up for you, but you will need to fill in a couple of missing pieces.\n",
    "- Train a neural network using only your PCA components as features.\n",
    "- Train a neural network using your PCA components and the predicted class membership from your clustering algorithm as features.\n",
    "- Compare and contrast how well each algorithm did. Which featurization technique would you pick and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we provide a template for training a neural network. Use this template for training on the original features, on the PCA components, and the PCA components + the predicted classes from your clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network on Original Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HSD010_binary based on HSD010\n",
    "nhanes['HSD010_binary'] = nhanes['HSD010'].replace(\n",
    "    [1, 2, 3, 4, 5], ['good', 'good', 'good', 'poor', 'poor']\n",
    ")\n",
    "\n",
    "# Encode binary target variable into 0 (poor) and 1 (good)\n",
    "nhanes['HSD010_binary_encoded'] = nhanes['HSD010_binary'].map({'good': 1, 'poor': 0})\n",
    "\n",
    "# Verify encoding\n",
    "print(nhanes[['HSD010', 'HSD010_binary', 'HSD010_binary_encoded']].head())\n",
    "\n",
    "# Step 4: Split data into features (X) and target (y)\n",
    "X = nhanes.drop(columns=['HSD010', 'HSD010_binary', 'HSD010_binary_encoded', 'SEQN'], errors='ignore')\n",
    "y = nhanes['HSD010_binary_encoded']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display sizes of the training and testing sets\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Save the training and testing sets\n",
    "X_train.to_csv(\"X_train.csv\", index=True)\n",
    "X_test.to_csv(\"X_test.csv\", index=True)\n",
    "y_train.to_csv(\"y_train.csv\", index=True)\n",
    "y_test.to_csv(\"y_test.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Ensure target variable and features are properly assigned\n",
    "y = nhanes['HSD010_binary_encoded']\n",
    "X = nhanes.drop(columns=['HSD010', 'HSD010_binary', 'HSD010_binary_encoded', 'SEQN'], errors='ignore')\n",
    "\n",
    "# Step 2: Handle missing values if any\n",
    "X = X.fillna(0)\n",
    "y = y.fillna(0)\n",
    "\n",
    "# Step 3: Convert to numeric if needed\n",
    "y = y.astype(int)  # Ensure target variable is numeric\n",
    "\n",
    "# Step 4: Verify shapes\n",
    "print(X.shape)  # Number of rows and columns in X\n",
    "print(y.shape)  # Length of y\n",
    "\n",
    "# Step 5: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10)\n",
    "\n",
    "# Step 6: Verify the split\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variable and features\n",
    "y = nhanes['HSD010_binary_encoded']  # Target variable\n",
    "X = nhanes.drop(columns=['HSD010', 'HSD010_binary', 'HSD010_binary_encoded', 'SEQN'], errors='ignore')  # Features\n",
    "\n",
    "# Perform train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10)\n",
    "\n",
    "# Check results\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Determine the number of unique classes\n",
    "num_classes = len(set(y_train))  # Replace `...` with the actual number of classes\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Print the results to verify\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# Ensure proper input data\n",
    "X_train = np.random.rand(100, 10)  # Example features\n",
    "y_train = np.random.randint(0, 2, 100)  # Binary labels (0 or 1)\n",
    "\n",
    "X_test = np.random.rand(20, 10)  # Example test features\n",
    "y_test = np.random.randint(0, 2, 20)  # Test binary labels\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=10,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network on Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accurracy is 64.99 percent. The performance is moderate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network on Principal Components + Cluster Membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I will use principal components because some data are correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In your own words, what is the difference between PCA and clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA - Reduces dimensionality by keeping the variance. It is variance-based not grouping-based. Its outputs are continuous variables. It looks for patterns in data.\n",
    "\n",
    "Clustering - It groups similar data. It is unsupervised. Its outputs are distant metrics. The output is different cluster labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Did you notice any advantages to combining PCA and clustering? If so, what do you think they were? If not, why do you think you didn't see any gains from this combination?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the clustering accuracy becomes better.\n",
    "Removes multicollinearity.\n",
    "Cluster separation is enhanced.\n",
    "It is easy to visualize the clusters.\n",
    "The clusters are easy to interpret.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How can unsupervised techniques help with downstream supervised learning tasks when working with \"big data?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dimension reduction takes place\n",
    "- Look in the data to find meaningful features\n",
    "- Identifies outliers\n",
    "- Boost supervised model performance\n",
    "- Grouping of data without labels into similar subsets\n",
    "- Handles irrelevant features\n",
    "- Better accuracy of neural networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
